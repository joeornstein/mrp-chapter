---
title: "Multilevel Regression And Poststratification (A Primer)"
author:
  - Joseph T. Ornstein^[Department of Political Science, University of Georgia]
# date: "November 15, 2021"
output: pdf_document
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
set.seed(42)
```

Often we'd like to estimate public opinion on some policy issue, but our surveys are unrepresentative in an important way. Perhaps their respondents come from a convenience sample [@wang2015], or non-response bias skews an otherwise random sample. Or perhaps the data is representative of some larger population (i.e. a country-level random sample), but contains too few observations to make inferences about a subgroup of interest. Even the largest US public opinion surveys do not have enough respondents to make reliable inferences about lower-level political entities like states or municipalities. Conclusions drawn from low frequency observations -- even in a large sample survey -- can be wildly misleading [@ansolabehere2015].

This presents a challenge for public opinion research: how to take unrepresentative survey data and adjust it so that it is useful for our particular research question. In this chapter, I will demonstrate a method called **multilevel regression and poststratification** (MRP). Using this approach, the researcher first constructs a model of public opinion (multilevel regression) and then reweights the model's predictions based on the observed characteristics of the population of interest (poststratification). In the sections to follow, I will describe this approach in detail, and will accompany this explanation with code in the `R` statistical language.

MRP was first introduced by @gelman1997, and the subsequent decades it has helped address a diverse set of research questions in political science. These range from generating election forecasts using unrepresentative survey data [@wang2015] to assessing the responsiveness of state [@lax2012] and local policymakers [@tausanovitch2014] to their constituents' policy preferences.

In the following sections, I will illustrate how MRP can improve estimates of small area public opinion. Our running example will be drawn from the Cooperative Election Study [@schaffner2021], which includes a question asking respondents whether they support a policy that would "decrease the number of police on the street by 10 percent, and increase funding for other public services." Since police reform is a policy issue on which US local governments have a significant amount of autonomy, it would be interesting to know how opinions on this issue vary from place to place without having to conduct a large set of costly local-level surveys.

As we will see, the accuracy of our poststratified estimates depends critically on whether the first-stage model makes good predictions. The best first-stage models are *regularized* [@gelman2018] to avoid both over- and under-fitting to the survey data. Regularized ensemble models [@ornstein2020] with group-level predictors tend to produce the best estimates, especially when trained on large datasets.

# Running Example

To demonstrate how MRP works, we'll consider an example where we know the true population-level estimands, and can explore how various refinements to the method can improve predictive accuracy. This approach mirrors @buttice2013, who use disaggregated ressponses from large-scale US survey of voters [@schaffner2021] as the target.[^1] The Cooperative Election Study data is available [here](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi%3A10.7910/DVN/E9N6PH), and we'll be using a tidied version of the dataset created by the `R/cleanup-ces-2020.R` script.

[^1]: Throughout, I will use R functions from the "tidyverse" to make the code more human-readable. All code will be available for download on a public repository.

```{r load-ces}
library(tidyverse)
library(ggrepel)

load('data/CES-2020.RData')
```

This tidied version of the data only includes the `r length(unique(ces$abb))` states with at least 500 respondents.

## The Truth

First, let's plot the percent of CES respondents who supported "defunding" the police[^2] by state.

[^2]: Obviously that phrase means different things to different people. In this case, we'll stick with the CES proposed policy of reducing police staffing by 10% and diverting those expenditures to other priorities.

```{r truth, fig.cap='The percent of CES respondents in each state who support reducing police budgets. These are our target estimands.'}
truth <- ces %>% 
  group_by(abb) %>% 
  summarize(truth = mean(defund_police))

# plot
truth %>% 
  # reorder abb so the chart is organized by percent who support
  mutate(abb = fct_reorder(abb, truth)) %>% 
  ggplot(mapping = aes(x=truth, y=abb)) +
  geom_point(alpha = 0.7) +
  labs(x = 'Percent Who Support Policy', y = 'State') +
  theme_minimal()
```

Note that these values likely overstate the percent of the total population that support such a policy, as self-identified Democrats are overrepresented in the CES sample. But they will nevertheless serve as the "truth" that we will try to estimate with MRP.

## Draw a Sample

Now suppose that we did not have access to the entire CES dataset, but only to a random sample of 1,000 respondents. How good of a job can we do at estimating those state-level means?

```{r sample}
sample_data <- ces %>% 
  slice_sample(n = 1000)
```

```{r disaggregate}
sample_summary <- sample_data %>% 
  group_by(abb) %>% 
  summarize(estimate = mean(defund_police),
            num = n())

sample_summary
```

In a sample with only 1,000 respondents, there are several states with very few (or no) respondents. Notice, for example, that this sample includes only four respondents from Arkansas, of whom zero support reducing police budgets. So simply disaggregating and taking sample means is unlikely to yield good estimates, as you can see by comparing those sample means against the truth.

```{r compare-disaggregation, fig.cap='Esimates from disaggregated sample data'}
# a function to plot the state-level estimates against the truth
compare_to_truth <- function(estimates, truth){
  
  d <- left_join(estimates, truth, by = 'abb')
  
  ggplot(data = d,
         mapping = aes(x=estimate,
                       y=truth,
                       label=abb)) +
  geom_point(alpha = 0.5) +
  geom_text_repel() +
  theme_minimal() +
  geom_abline(intercept = 0, slope = 1, linetype = 'dashed') +
  labs(x = 'Estimate',
       y = 'Truth',
       caption = paste0('Correlation = ', round(cor(d$estimate, d$truth), 2)))
}

compare_to_truth(sample_summary, truth)
```

This is clearly a poor estimate of state-level public opinion. The four respondents from Arksansas simply do not give us enough information to adequately measure public opinion in Arkansas. But one of the key insights behind MRP is that the respondents from Arkansas are not the only respondents who can give us information about Arkansas! There are other respondents in, for example, Missouri, that are similar to Arkansas residents on their observed characteristics. If we can determine the characteristics that predict support for police reform using the entire survey, then we can use those predictions -- combined with demographic information about Arkansans -- to generate better estimates. The trick, in essence, is that our estimate for Arkansas will be borrowing information from similar respondents in other states.

MRP proceeds in two steps.

## Step 1: Fit a Model

First, we fit a model of our outcome, using observed characteristics of the survey respondents as predictors. As a first pass, let's fit a simple logistic model of including only four demographic predictors: gender, education, race, and age.

```{r model}
model <- glm(defund_police ~
              gender + educ + race + age,
            data = sample_data,
            family = 'binomial')
```

## Step 2: Construct the Poststratification Frame

The poststratification stage requires the researcher to know (or estimate) the joint frequency distribution of predictor variables. This information is stored in a "poststratification frame", a matrix where each row is a unique combination of characteristics, along with the frequency of that combination. Often, one can construct these frequency distributions from Census micro-data [@lax2009]. For our example, I will compute them directly from the CES.

```{r psframe}
psframe <- ces %>% 
  count(abb, gender, educ, race, age)

head(psframe)
```

## Step 3: Predict and Poststratify

With the model and poststratification frame in hand, the final step is to generate frequency-weighted predictions of public opinion. For each cell in the poststratification frame, append the model's predicted probability of supporting police defunding.

```{r predict}
psframe$predicted_probability <- predict(model, psframe, type = 'response')
```

Then the poststratified estimates are the frequency-weighted means of those predictions.

```{r poststratify}
poststratified_estimates <- psframe %>% 
  group_by(abb) %>% 
  summarize(estimate = weighted.mean(predicted_probability, n))
```

Let's see how these estimates compare with the known values.

```{r compare-to-truth, fig.cap='Underfit MRP estimates from complete pooling model'}
compare_to_truth(poststratified_estimates, truth)
```

These estimates, though still imperfect, are much better than the previous estimates from disaggregation. Notice, in particular, that the estimate for Arkansas went from 0% to roughly 39%, reflecting the significant improvement that comes from using more information than the four Arkansans in our sample can provide.

But we can still make improvements.

This highlights one of the dangers of producing MRP estimates from an underspecified model. When the first-stage model is underfit, poststratified estimates tend to overshrink towards the global mean.

This compression means we should be wary of MRP studies that show policy outcomes "leapfrogging" estimated public opinion [@simonovits2020]. It could be that policymakers are more extreme than their constituents, or that MRP produces estimates of constituency preferences that are too moderate.

## The Other Extreme: Overfitting

To illustrate the other extreme, let's estimate a model with a separate intercept term for each state -- a "fixed effects" model. Because our sample contains several states with very few observations, these state-specific intercepts will likely overfit to sampling variability.

```{r, fig.cap='Overfit MRP estimates from fixed effects model'}

# fit the model
model2 <- glm(defund_police ~
              gender + educ + race + age +
                abb,
            data = sample_data,
            family = 'binomial')

# make predictions
psframe <- psframe %>% 
  mutate(predicted_probability = predict(model2, psframe, type = 'response'))

# poststratify
poststratified_estimates <- psframe %>% 
  group_by(abb) %>% 
  summarize(estimate = weighted.mean(predicted_probability, n))


compare_to_truth(poststratified_estimates, truth)
```

Compare this to Figure 1 -- these estimates are similarly overfit. Iowa is predicted to have roughly 100% support due to an idiosyncratic sample, while Maryland has the opposite problem.

## The Sweet Spot: Partial Pooling

@gelman1997 solution: multilevel models that partially pool across regions. (Explanation of partial pooling goes...here)

```{r, fig.cap='MRP estimates from model with partial pooling'}
library(lme4)

model3 <- glmer(defund_police ~  gender + educ + race + age +
                (1|abb), 
                 data = sample_data,
                 family = 'binomial')

# make predictions
psframe <- psframe %>% 
  mutate(predicted_probability = predict(model3, psframe, type = 'response'))

# poststratify
poststratified_estimates <- psframe %>% 
  group_by(abb) %>% 
  summarize(estimate = weighted.mean(predicted_probability, n))


compare_to_truth(poststratified_estimates, truth)
```

TODO: Well that's not the approach we should take, then. Partial pooling isn't magic. It just undoes the damage that fixed effects does. The magic is in good geographic predictors.

## The Importance of Group-Level Covariates

-   The Democratic presidential two party vote share in 2020

-   The percent of the state's residents that live in urban areas

-   The state's 2020 homicide rate (homicides per 10,000 residents)

-   

## Stacked Regression and Poststratification (SRP)

Ultimately, the accuracy of one's poststratified estimates depends on the out-of-sample predictive performance of the first-stage model. As we've seen above, the challenge is to thread the needle between overfitting and underfitting. Several recent papers [@bisbee2019, @ornstein2020, @broniecki2022] have shown that models from machine learning can help automate this process.

In the code below, I'll demonstrate how an *ensemble* of models -- using the same set of predictors but different methods for combining them into predictions -- can yield superior performance to a single multilevel regression model. In particular, I will fit a "Super Learner" ensemble [@vanderlaan2007], which generates a weighted average prediction from multiple models, where the weights are based on cross-validated prediction performance. The literature on ensemble models is vast, but for good entry points I recommend @breiman1996, @breiman2001, and @montgomery2012.

```{r, fig.cap='Estimates from an ensemble first-stage model'}

library(SuperLearner)

# fit Super Learner
SL.library <- c("SL.ranger", "SL.xgboost", "SL.glm")

X <- model.matrix(~gender + educ + race+ age + abb, sample_data)

newX <- model.matrix(~gender + educ + race+ age + abb, psframe)

sl <- SuperLearner(Y = sample_data$defund_police,
                       X = X, 
                       newX = newX, 
                       family = binomial(),
                       SL.library = SL.library, verbose = TRUE)
#sl$SL.predict
sl$coef

# make predictions
psframe$predicted_probability <- sl$SL.predict

# poststratify
poststratified_estimates <- psframe %>% 
  group_by(abb) %>% 
  summarize(estimate = weighted.mean(predicted_probability, n))

compare_to_truth(poststratified_estimates, truth)
```

And that's just "out-of-the-box"! What if we were more careful about it?\

This reflects the gains that come from modeling "deep interactions" in the predictors of public opinion [@ghitza2013]. If, for example, income better predicts partisanship in some states but not in others [@gelman2007], then a model that captures that moderating effect will produce better poststratified estimates than one that does not. Machine learning techniques like random forest [@breiman2001] are especially useful for detecting and modeling such deep interactions, and stacked regression and poststratification (SRP) tends to outperform MRP in simulations, particularly for training data with large sample size [@ornstein2020].

## Synthetic Poststratification

Suppose that we did not have access to the entire joint distribution of individual-level covariates. @leemann2017 suggest an extension of Mr. P, which they (delightfully) dub Multilevel Regression and Synthetic Poststratification (Mrs. P). Lacking the full joint distribution of covariates for poststratification, one can instead create a *synthetic* poststratification frame by assuming that additional covariates are statistically independent of one another. So long as your first stage model is linear-additive, this approach yields the same predictions as if you knew the true joint distribution![^3] And even if the first-stage model is not linear-additive, simulations suggest that the improved performance from additional predictors tends to overcome the error introduced by synthetic poststratification.

[^3]: See @ornstein2020 appendix A for mathematical proof.

To create a synthetic poststratification frame, convert frequencies to probabilities and multiply. For example, suppose we only had the joint distribution for gender, race, and age, and wanted to create a synthetic poststratification including education.

Add:

-   How important is religion to the respondent?

-   Whether the respondent lives in an urban, rural, or suburban area

-   Whether the respondent or a member of the respondent's family is a military veteran

-   Whether the respondent owns or rents their home

-   Is the respondent the parent or guardian of a child under the age of 18?

These variables may be useful predictors of opinion about police policy and the first-stage model could be improved by including them. But there is no dataset (that I know of) that would allow us to compute a state-level joint probability distribution over every one of them. Instead, we would typically only know the marginal distributions of each covariate (e.g. the percent of a state's residents that are military households, or the percent that live in urban areas). (TODO: Flip this up top to motivate synthetic poststratification)

```{r synthetic-psframe}
# poststratification frame with 3 variables
psframe3 <- ces %>% 
  count(abb, gender, race, age) %>% 
  group_by(abb) %>% 
  mutate(prob = n / sum(n))

head(psframe3)

# distribution of education variable by state
psframe_educ <- ces %>% 
  count(abb, educ) %>% 
  group_by(abb) %>% 
  mutate(prob2 = n / sum(n))

head(psframe_educ)

synthetic_psframe <- left_join(psframe3, psframe_educ,
                               by = 'abb') %>% 
  mutate(prob = prob * prob2)

head(synthetic_psframe)
```

The `SRP` package contains a convenience function for this operation (see the [vignette](https://joeornstein.github.io/software/SRP/) for more information).

Then poststratify as normal.

```{r}
# make predictions
synthetic_psframe$predicted_probability <- predict(model3, synthetic_psframe, type = 'response')

# poststratify
poststratified_estimates <- synthetic_psframe %>% 
  group_by(abb) %>% 
  summarize(estimate = weighted.mean(predicted_probability, prob))


compare_to_truth(poststratified_estimates, truth)
```

Note that the performance is slightly worse than when we knew the true joint distribution. But is it worse than omitting education entirely?

```{r, fig.cap='Poststratified estimates, omitting education'}

model4 <- glmer(defund_police ~  gender + race + age +
                (1|abb), 
                 data = sample_data,
                 family = 'binomial')


# make predictions
psframe3$predicted_probability <- predict(model4, psframe3, type = 'response')

# poststratify
poststratified_estimates <- psframe3 %>% 
  group_by(abb) %>% 
  summarize(estimate = weighted.mean(predicted_probability, prob))


compare_to_truth(poststratified_estimates, truth)
```

## Best Performing

Supposing we had access to the true joint distribution and fit an ensemble first-stage model...

```{r, fig.cap='The best performing estimates, using an ensemble first stage model, group-level predictors, and synthetic poststratification.'}
psframe <- ces %>% 
  count(abb, gender, race, age, educ,
        pew_religimp, homeowner, urban, 
        parent, military_household,
        biden_vote_share, homicide_rate)

library(SuperLearner)

# fit Super Learner
SL.library <- c("SL.ranger", "SL.glm")

X <- sample_data %>% 
  select(abb, gender, race, age, educ,
        pew_religimp, homeowner, urban, 
        parent, military_household,
        biden_vote_share, homicide_rate)

newX <- psframe %>% 
  select(abb, gender, race, age, educ,
        pew_religimp, homeowner, urban, 
        parent, military_household,
        biden_vote_share, homicide_rate)

sl <- SuperLearner(Y = sample_data$defund_police,
                       X = X, 
                       newX = newX, 
                       family = binomial(),
                       SL.library = SL.library, 
                       verbose = FALSE)
#sl$SL.predict
sl$coef

# make predictions
psframe$predicted_probability <- sl$SL.predict

# poststratify
poststratified_estimates <- psframe %>% 
  group_by(abb) %>% 
  summarize(estimate = weighted.mean(predicted_probability, n))

compare_to_truth(poststratified_estimates, truth)
```

# Conclusion

In the code above I have emphasized "do-it-yourself" approaches to MRP -- fitting a model, building a poststratification frame, and producing estimates separately. But there are a number of `R` packages available with useful functions to help ease the process. In particular, I would encourage curious readers to explore the `autoMrP` package [@broniecki2022], which implements the ensemble modeling approach described above, and performs quite well in simulations when compared to existing packages.

# References
